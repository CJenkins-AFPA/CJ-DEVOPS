# Lab 2 - Haute DisponibilitÃ© et Persistance

## ğŸ¯ Objectifs pÃ©dagogiques

- ImplÃ©menter un cluster multi-managers pour la haute disponibilitÃ©
- MaÃ®triser la gestion des volumes distribuÃ©s
- Configurer la persistance des donnÃ©es critiques
- GÃ©rer les secrets et configurations sensibles
- Mettre en place des stratÃ©gies de backup et restore

## ğŸ“‹ PrÃ©requis

- Lab 1 complÃ©tÃ© et validÃ©
- Cluster Swarm opÃ©rationnel (1 manager + 2 workers)
- ComprÃ©hension des concepts de base de Swarm
- AccÃ¨s SSH Ã  toutes les machines

## ğŸ—ï¸ Architecture cible

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CLUSTER HAUTE DISPONIBILITÃ‰                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  manager1   â”‚   â”‚  manager2   â”‚   â”‚  manager3   â”‚          â”‚
â”‚  â”‚  (Leader)   â”‚â—„â”€â–ºâ”‚  (Follower) â”‚â—„â”€â–ºâ”‚  (Follower) â”‚          â”‚
â”‚  â”‚ 192.168.56  â”‚   â”‚ 192.168.56  â”‚   â”‚ 192.168.56  â”‚          â”‚
â”‚  â”‚     .10     â”‚   â”‚     .20     â”‚   â”‚     .30     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚         â”‚                  â”‚                  â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                       RAFT CONSENSUS                            â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚   worker1   â”‚   â”‚   worker2   â”‚                             â”‚
â”‚  â”‚192.168.56.11â”‚   â”‚192.168.56.12â”‚                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                                                  â”‚
â”‚  [Volumes DistribuÃ©s] [Secrets] [Configs]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š Exercices

### Exercice 2.1 - Promotion de Workers en Managers

**Objectif** : CrÃ©er un cluster avec 3 managers pour la haute disponibilitÃ©

**ThÃ©orie** :
- Nombre optimal de managers : 3, 5 ou 7
- Quorum = (N/2) + 1
- Pour 3 managers : tolÃ¨re 1 panne
- Pour 5 managers : tolÃ¨re 2 pannes

**Ã‰tapes** :

1. Ã‰tat initial du cluster :
```bash
docker node ls
```

2. Promouvoir worker1 en manager :
```bash
docker node promote worker1
```

3. Ajouter un nouveau nÅ“ud manager3 :
```bash
# Sur la nouvelle VM manager3
vagrant ssh manager3

# RÃ©cupÃ©rer le token manager depuis manager1
docker swarm join-token manager

# Joindre le cluster en tant que manager
docker swarm join --token SWMTKN-1-xxxxx-manager 192.168.56.10:2377
```

4. VÃ©rification :
```bash
docker node ls
# VÃ©rifier que vous avez 3 managers
```

**Questions** :
- Combien de pannes le cluster peut-il tolÃ©rer maintenant ?
- Que signifie le statut "Reachable" vs "Leader" ?
- Quel est l'algorithme utilisÃ© pour l'Ã©lection du leader ?

**Livrables** :
- Screenshot de `docker node ls` montrant 3 managers
- Document expliquant le quorum Raft

---

### Exercice 2.2 - Test de Failover Manager

**Objectif** : Valider le basculement automatique du leader

**Ã‰tapes** :

1. Identifier le leader actuel :
```bash
docker node ls
# Noter quel nÅ“ud a le statut "Leader"
```

2. ArrÃªter le leader (simuler une panne) :
```bash
# Si manager1 est leader
vagrant ssh manager1
sudo systemctl stop docker
```

3. Observer depuis un autre manager :
```bash
vagrant ssh manager2
watch -n 1 docker node ls
```

4. ChronomÃ©trer :
- Temps de dÃ©tection de la panne
- Temps d'Ã©lection du nouveau leader
- Temps total de basculement

5. RedÃ©marrer le manager :
```bash
vagrant ssh manager1
sudo systemctl start docker
```

**Questions** :
- Combien de temps a pris l'Ã©lection du nouveau leader ?
- Le cluster a-t-il continuÃ© de fonctionner pendant le basculement ?
- L'ancien leader redevient-il automatiquement leader au redÃ©marrage ?

**Tests supplÃ©mentaires** :
```bash
# Tester avec un service en cours
docker service create --name test-ha --replicas 5 nginx:alpine

# ArrÃªter le leader et vÃ©rifier que le service fonctionne toujours
```

**Livrables** :
- Chronologie dÃ©taillÃ©e du failover
- Analyse de la disponibilitÃ© du service pendant le basculement

---

### Exercice 2.3 - Volumes Locaux et Contraintes

**Objectif** : Comprendre les limites des volumes locaux en Swarm

**ProblÃ©matique** :
Les volumes Docker locaux ne se dÃ©placent pas avec les conteneurs

**DÃ©monstration** :

1. CrÃ©er un service avec volume local :
```bash
docker service create \
  --name db-local \
  --mount type=volume,source=mydata,target=/data \
  --constraint 'node.hostname==worker1' \
  postgres:15-alpine
```

2. Ã‰crire des donnÃ©es :
```bash
# Trouver le conteneur
docker ps

# Se connecter et crÃ©er des donnÃ©es
docker exec -it <container_id> psql -U postgres
CREATE DATABASE testdb;
\q
```

3. Retirer la contrainte et observer :
```bash
docker service update --constraint-rm 'node.hostname==worker1' db-local

# Le conteneur se dÃ©place mais perd ses donnÃ©es !
```

**Questions** :
- Que devient le volume sur worker1 ?
- Pourquoi les donnÃ©es ne suivent-elles pas le conteneur ?
- Quelles solutions existent pour ce problÃ¨me ?

**Livrables** :
- Documentation du comportement observÃ©
- Analyse des cas d'usage appropriÃ©s pour les volumes locaux

---

### Exercice 2.4 - Solutions de Stockage DistribuÃ©

**Objectif** : ImplÃ©menter des solutions de persistance adaptÃ©es Ã  Swarm

**Option A : NFS PartagÃ©**

1. Configuration du serveur NFS (sur manager1) :
```bash
# Installer NFS
sudo apt-get update
sudo apt-get install -y nfs-kernel-server

# CrÃ©er le rÃ©pertoire partagÃ©
sudo mkdir -p /srv/nfs/swarm-data
sudo chown nobody:nogroup /srv/nfs/swarm-data
sudo chmod 777 /srv/nfs/swarm-data

# Configurer les exports
echo "/srv/nfs/swarm-data 192.168.56.0/24(rw,sync,no_subtree_check,no_root_squash)" | sudo tee -a /etc/exports

# Appliquer la configuration
sudo exportfs -ra
sudo systemctl restart nfs-kernel-server
```

2. Configuration des clients NFS (sur tous les nÅ“uds) :
```bash
# Installer le client NFS
sudo apt-get install -y nfs-common

# CrÃ©er le point de montage
sudo mkdir -p /mnt/nfs/swarm-data

# Monter le partage
sudo mount 192.168.56.10:/srv/nfs/swarm-data /mnt/nfs/swarm-data

# Rendre permanent
echo "192.168.56.10:/srv/nfs/swarm-data /mnt/nfs/swarm-data nfs defaults 0 0" | sudo tee -a /etc/fstab
```

3. Utilisation avec Docker :
```bash
docker service create \
  --name db-nfs \
  --mount type=bind,source=/mnt/nfs/swarm-data,target=/var/lib/postgresql/data \
  --replicas 1 \
  postgres:15-alpine
```

**Option B : Plugin de Volume (Rex-Ray)**

```bash
# Installer Rex-Ray sur tous les nÅ“uds
curl -sSL https://dl.bintray.com/emccode/rexray/install | sh

# Configuration (exemple pour NFS)
sudo tee /etc/rexray/config.yml << EOF
libstorage:
  service: nfs
nfs:
  host: 192.168.56.10
  volumePath: /srv/nfs/volumes
EOF

# DÃ©marrer Rex-Ray
sudo systemctl start rexray
sudo systemctl enable rexray

# CrÃ©er un volume
docker volume create -d rexray -o size=1 --name shared-data

# Utiliser le volume
docker service create \
  --name db-rexray \
  --mount source=shared-data,target=/var/lib/postgresql/data \
  postgres:15-alpine
```

**Questions** :
- Quels sont les avantages et inconvÃ©nients de chaque approche ?
- Quelle solution convient le mieux pour une base de donnÃ©es ?
- Comment gÃ©rer les permissions ?

**Livrables** :
- Configuration complÃ¨te de NFS
- Tests de mobilitÃ© des conteneurs avec donnÃ©es persistantes
- Comparatif des solutions

---

### Exercice 2.5 - Stack Applicative avec Persistance

**Objectif** : DÃ©ployer WordPress avec base de donnÃ©es persistante

**Fichier** : `wordpress-stack.yml`

```yaml
version: '3.8'

services:
  db:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password
      MYSQL_DATABASE: wordpress
      MYSQL_USER: wordpress
      MYSQL_PASSWORD_FILE: /run/secrets/db_password
    secrets:
      - db_root_password
      - db_password
    volumes:
      - db-data:/var/lib/mysql
    networks:
      - backend
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == nfs
      restart_policy:
        condition: on-failure

  wordpress:
    image: wordpress:latest
    ports:
      - "8080:80"
    environment:
      WORDPRESS_DB_HOST: db:3306
      WORDPRESS_DB_USER: wordpress
      WORDPRESS_DB_PASSWORD_FILE: /run/secrets/db_password
      WORDPRESS_DB_NAME: wordpress
    secrets:
      - db_password
    volumes:
      - wp-content:/var/www/html/wp-content
    networks:
      - backend
      - frontend
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
    depends_on:
      - db

networks:
  frontend:
    driver: overlay
  backend:
    driver: overlay

volumes:
  db-data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.56.10,rw
      device: ":/srv/nfs/swarm-data/mysql"
  
  wp-content:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.56.10,rw
      device: ":/srv/nfs/swarm-data/wordpress"

secrets:
  db_root_password:
    external: true
  db_password:
    external: true
```

**PrÃ©paration** :

```bash
# CrÃ©er les rÃ©pertoires NFS
sudo mkdir -p /srv/nfs/swarm-data/mysql
sudo mkdir -p /srv/nfs/swarm-data/wordpress

# Ajouter un label au nÅ“ud pour le stockage
docker node update --label-add storage=nfs worker1

# CrÃ©er les secrets
echo "MyRootPassword123!" | docker secret create db_root_password -
echo "MyWordPressPassword123!" | docker secret create db_password -

# DÃ©ployer la stack
docker stack deploy -c wordpress-stack.yml wp
```

**VÃ©rification** :

```bash
docker stack services wp
docker stack ps wp
```

**Tests** :
1. Installer WordPress via http://192.168.56.10:8080
2. CrÃ©er un article
3. Supprimer le service db et le recrÃ©er
4. VÃ©rifier que les donnÃ©es sont toujours lÃ 

**Questions** :
- Les donnÃ©es survivent-elles Ã  la suppression du conteneur ?
- Que se passe-t-il si vous scalez WordPress Ã  5 rÃ©plicas ?
- Comment gÃ©rer les uploads de mÃ©dias ?

**Livrables** :
- Stack YAML fonctionnelle
- Tests de persistance documentÃ©s
- Analyse des points de vigilance

---

### Exercice 2.6 - Gestion AvancÃ©e des Secrets

**Objectif** : SÃ©curiser les donnÃ©es sensibles dans Swarm

**Concepts** :
- Les secrets sont chiffrÃ©s au repos et en transit
- StockÃ©s dans le Raft log (managers uniquement)
- MontÃ©s en RAM dans les conteneurs (/run/secrets/)
- Jamais Ã©crits sur disque dans les conteneurs

**Pratique** :

1. CrÃ©er diffÃ©rents types de secrets :
```bash
# Secret depuis string
echo "MonSuperMotDePasse" | docker secret create mysql_root_password -

# Secret depuis fichier
echo "user:password:1000:1000:User Name:/home/user:/bin/bash" > passwd
docker secret create passwd_file passwd
rm passwd

# Secret depuis variable d'environnement
export DB_PASS="SecurePassword123"
echo "$DB_PASS" | docker secret create db_password -
```

2. Lister et inspecter :
```bash
docker secret ls
docker secret inspect mysql_root_password
# Note: le contenu n'est PAS visible
```

3. Utiliser les secrets :
```yaml
version: '3.8'

services:
  app:
    image: myapp:latest
    secrets:
      - source: db_password
        target: /run/secrets/db_pass
        mode: 0400
    environment:
      DB_PASSWORD_FILE: /run/secrets/db_pass
```

4. Rotation de secrets :
```bash
# CrÃ©er une nouvelle version
echo "NewPassword456" | docker secret create db_password_v2 -

# Mettre Ã  jour le service
docker service update \
  --secret-rm db_password \
  --secret-add db_password_v2 \
  myapp

# Supprimer l'ancien secret
docker secret rm db_password
```

**Questions** :
- OÃ¹ sont stockÃ©s les secrets physiquement ?
- Comment un worker accÃ¨de-t-il aux secrets ?
- Peut-on modifier un secret existant ?

**Exercice pratique** :
CrÃ©er une application multi-tiers avec :
- Secret pour la base de donnÃ©es
- Secret pour une clÃ© API
- Secret pour un certificat SSL

**Livrables** :
- ProcÃ©dure de gestion des secrets
- Exemple de rotation de secrets
- Bonnes pratiques documentÃ©es

---

### Exercice 2.7 - Configurations Dynamiques

**Objectif** : GÃ©rer les configurations applicatives avec Docker Config

**DiffÃ©rence Secret vs Config** :
- **Secret** : donnÃ©es sensibles (mots de passe, clÃ©s)
- **Config** : donnÃ©es non sensibles (fichiers de config)

**Pratique** :

1. CrÃ©er une configuration Nginx :
```bash
cat > nginx.conf << 'EOF'
server {
    listen 80;
    server_name example.com;
    
    location / {
        proxy_pass http://backend:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
EOF

docker config create nginx_config nginx.conf
```

2. Utiliser la configuration :
```yaml
version: '3.8'

services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    configs:
      - source: nginx_config
        target: /etc/nginx/conf.d/default.conf
    networks:
      - frontend

configs:
  nginx_config:
    external: true

networks:
  frontend:
    driver: overlay
```

3. Mise Ã  jour de configuration :
```bash
# CrÃ©er une nouvelle version
cat > nginx-v2.conf << 'EOF'
server {
    listen 80;
    server_name example.com;
    
    location / {
        proxy_pass http://backend:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        
        # Nouvelles options
        proxy_cache_bypass $http_upgrade;
        proxy_http_version 1.1;
    }
}
EOF

docker config create nginx_config_v2 nginx-v2.conf

# Mettre Ã  jour le service
docker service update \
  --config-rm nginx_config \
  --config-add source=nginx_config_v2,target=/etc/nginx/conf.d/default.conf \
  nginx
```

**Cas d'usage avancÃ©** : Application multi-environnement

```bash
# Config pour dÃ©veloppement
docker config create app_config_dev app-dev.yml

# Config pour production
docker config create app_config_prod app-prod.yml

# DÃ©ployer selon l'environnement
docker service create \
  --name myapp \
  --config source=app_config_prod,target=/app/config.yml \
  myapp:latest
```

**Questions** :
- Quelle est la taille maximale d'une config ?
- Peut-on partager une config entre plusieurs services ?
- Comment versionner les configs ?

**Livrables** :
- Exemples de configurations pour diffÃ©rents services
- ProcÃ©dure de mise Ã  jour sans interruption
- StratÃ©gie de versioning

---

### Exercice 2.8 - Backup et Restore du Swarm

**Objectif** : Sauvegarder et restaurer l'Ã©tat du cluster

**Importance** :
- Sauvegarder les donnÃ©es du Raft (secrets, configs, services)
- Plan de disaster recovery
- Migration de cluster

**ProcÃ©dure de Backup** :

1. Backup sur un manager :
```bash
# ArrÃªter Docker sur le manager
sudo systemctl stop docker

# Sauvegarder le rÃ©pertoire Swarm
sudo tar -czvf swarm-backup-$(date +%Y%m%d).tar.gz \
  /var/lib/docker/swarm

# RedÃ©marrer Docker
sudo systemctl start docker

# Sauvegarder aussi les volumes (si locaux)
sudo tar -czvf volumes-backup-$(date +%Y%m%d).tar.gz \
  /var/lib/docker/volumes
```

2. Script de backup automatisÃ© :
```bash
#!/bin/bash
# backup-swarm.sh

BACKUP_DIR="/backup/swarm"
DATE=$(date +%Y%m%d-%H%M%S)

# CrÃ©er le rÃ©pertoire de backup
mkdir -p $BACKUP_DIR

# Backup du Raft
sudo systemctl stop docker
sudo tar -czf $BACKUP_DIR/swarm-$DATE.tar.gz /var/lib/docker/swarm
sudo systemctl start docker

# Backup des secrets (export)
docker secret ls -q | while read secret; do
    echo "Secret: $secret" >> $BACKUP_DIR/secrets-list-$DATE.txt
done

# Backup des configs
docker config ls -q | while read config; do
    docker config inspect $config > $BACKUP_DIR/config-$config-$DATE.json
done

# Backup de la topologie
docker node ls --format "{{.ID}} {{.Hostname}} {{.Status}} {{.Availability}}" \
  > $BACKUP_DIR/nodes-$DATE.txt

# Backup des services
docker service ls --format "{{.ID}} {{.Name}} {{.Replicas}}" \
  > $BACKUP_DIR/services-$DATE.txt

# Nettoyer les backups de plus de 7 jours
find $BACKUP_DIR -name "*.tar.gz" -mtime +7 -delete

echo "Backup completed: $DATE"
```

**ProcÃ©dure de Restore** :

1. Restore complet :
```bash
# Sur un nouveau manager
sudo systemctl stop docker

# Restaurer les donnÃ©es
sudo rm -rf /var/lib/docker/swarm
sudo tar -xzvf swarm-backup-YYYYMMDD.tar.gz -C /

# RedÃ©marrer Docker
sudo systemctl start docker

# Forcer la rÃ©initialisation
docker swarm init --force-new-cluster --advertise-addr 192.168.56.10

# Rejoindre les autres managers et workers
```

**Questions** :
- Quelle est la frÃ©quence de backup recommandÃ©e ?
- Peut-on faire un backup Ã  chaud ?
- Comment tester la procÃ©dure de restore ?

**Exercice** :
1. CrÃ©er un cluster avec quelques services
2. Faire un backup complet
3. DÃ©truire complÃ¨tement le cluster
4. Restaurer depuis le backup
5. VÃ©rifier que tout fonctionne

**Livrables** :
- Script de backup automatisÃ©
- ProcÃ©dure de restore documentÃ©e
- RÃ©sultats d'un test de restore

---

### Exercice 2.9 - Healthchecks et Auto-Healing

**Objectif** : Configurer la surveillance automatique des services

**Concepts** :
- Healthcheck au niveau de l'image Docker
- Healthcheck au niveau du service Swarm
- Actions automatiques en cas de problÃ¨me

**Healthcheck dans Dockerfile** :

```dockerfile
FROM nginx:alpine

# Installer curl pour le healthcheck
RUN apk add --no-cache curl

# Configuration du healthcheck
HEALTHCHECK --interval=30s \
            --timeout=3s \
            --start-period=5s \
            --retries=3 \
  CMD curl -f http://localhost/ || exit 1

COPY index.html /usr/share/nginx/html/
```

**Healthcheck dans Docker Compose/Stack** :

```yaml
version: '3.8'

services:
  web:
    image: nginx:alpine
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 30s
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 40s

  api:
    image: myapi:latest
    deploy:
      replicas: 5
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  database:
    image: postgres:15
    deploy:
      replicas: 1
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
```

**Test de l'auto-healing** :

1. DÃ©ployer un service avec healthcheck :
```bash
docker service create \
  --name test-health \
  --replicas 3 \
  --health-cmd "curl -f http://localhost/ || exit 1" \
  --health-interval 10s \
  --health-retries 3 \
  --health-timeout 5s \
  --health-start-period 10s \
  nginx:alpine
```

2. Simuler une panne :
```bash
# Trouver un conteneur
docker ps | grep test-health

# Corrompre le healthcheck
docker exec <container_id> sh -c "rm /usr/share/nginx/html/index.html"

# Observer le comportement
watch -n 1 docker service ps test-health
```

3. Observer :
- Le conteneur devient "unhealthy"
- Swarm le redÃ©marre automatiquement
- Un nouveau conteneur sain le remplace

**Healthcheck avancÃ© pour API** :

```python
# app.py
from flask import Flask, jsonify
import psycopg2
import redis

app = Flask(__name__)

@app.route('/health')
def health():
    checks = {
        'status': 'healthy',
        'checks': {}
    }
    
    # Check database
    try:
        conn = psycopg2.connect("dbname=mydb user=user password=pass host=db")
        conn.close()
        checks['checks']['database'] = 'ok'
    except:
        checks['checks']['database'] = 'fail'
        checks['status'] = 'unhealthy'
    
    # Check Redis
    try:
        r = redis.Redis(host='redis', port=6379)
        r.ping()
        checks['checks']['redis'] = 'ok'
    except:
        checks['checks']['redis'] = 'fail'
        checks['status'] = 'unhealthy'
    
    status_code = 200 if checks['status'] == 'healthy' else 503
    return jsonify(checks), status_code

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

**Questions** :
- Quelle est la diffÃ©rence entre `interval` et `timeout` ?
- Que fait `start_period` ?
- Comment Ã©viter les faux positifs ?

**Livrables** :
- Stack avec healthchecks configurÃ©s
- Tests d'auto-healing documentÃ©s
- Bonnes pratiques pour les healthchecks

---

### Exercice 2.10 - Stack Production ComplÃ¨te

**Objectif** : Assembler tous les concepts dans une stack production-ready

**Application** : E-commerce avec microservices

**Architecture** :
- Frontend (React)
- API Gateway (Nginx)
- Service Produits (Node.js)
- Service Commandes (Python)
- Service Utilisateurs (Go)
- Base de donnÃ©es PostgreSQL
- Cache Redis
- File de messages RabbitMQ

**Fichier** : `ecommerce-stack.yml`

```yaml
version: '3.8'

services:
  # Frontend
  frontend:
    image: ecommerce/frontend:latest
    ports:
      - "80:80"
    configs:
      - source: nginx_config
        target: /etc/nginx/nginx.conf
    networks:
      - frontend
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
      restart_policy:
        condition: on-failure
      labels:
        - "app=ecommerce"
        - "tier=frontend"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  # API Gateway
  api-gateway:
    image: nginx:alpine
    ports:
      - "8080:80"
    configs:
      - source: gateway_config
        target: /etc/nginx/nginx.conf
    networks:
      - frontend
      - backend
    deploy:
      replicas: 2
      labels:
        - "app=ecommerce"
        - "tier=gateway"

  # Service Produits
  products-service:
    image: ecommerce/products:latest
    environment:
      DATABASE_URL_FILE: /run/secrets/db_url
      REDIS_URL: redis://redis:6379
    secrets:
      - db_url
    networks:
      - backend
    deploy:
      replicas: 3
      placement:
        constraints:
          - node.role == worker
      labels:
        - "app=ecommerce"
        - "tier=backend"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 20s
      timeout: 5s
      retries: 3

  # Service Commandes
  orders-service:
    image: ecommerce/orders:latest
    environment:
      DATABASE_URL_FILE: /run/secrets/db_url
      RABBITMQ_URL_FILE: /run/secrets/rabbitmq_url
    secrets:
      - db_url
      - rabbitmq_url
    networks:
      - backend
    deploy:
      replicas: 3
      labels:
        - "app=ecommerce"
        - "tier=backend"
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://localhost:5000/health || exit 1"]
      interval: 20s

  # Service Utilisateurs
  users-service:
    image: ecommerce/users:latest
    environment:
      DATABASE_URL_FILE: /run/secrets/db_url
    secrets:
      - db_url
      - jwt_secret
    networks:
      - backend
    deploy:
      replicas: 2
      labels:
        - "app=ecommerce"
        - "tier=backend"

  # Base de donnÃ©es
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: ecommerce
      POSTGRES_USER_FILE: /run/secrets/db_user
      POSTGRES_PASSWORD_FILE: /run/secrets/db_password
    secrets:
      - db_user
      - db_password
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - backend
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == nfs
      labels:
        - "app=ecommerce"
        - "tier=database"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Cache Redis
  redis:
    image: redis:7-alpine
    command: redis-server --requirepass ${REDIS_PASSWORD}
    networks:
      - backend
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      labels:
        - "app=ecommerce"
        - "tier=cache"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  # RabbitMQ
  rabbitmq:
    image: rabbitmq:3-management-alpine
    environment:
      RABBITMQ_DEFAULT_USER_FILE: /run/secrets/rabbitmq_user
      RABBITMQ_DEFAULT_PASS_FILE: /run/secrets/rabbitmq_password
    secrets:
      - rabbitmq_user
      - rabbitmq_password
    ports:
      - "15672:15672"  # Management UI
    networks:
      - backend
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    deploy:
      replicas: 1
      labels:
        - "app=ecommerce"
        - "tier=messaging"
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5

networks:
  frontend:
    driver: overlay
    attachable: true
  backend:
    driver: overlay
    driver_opts:
      encrypted: "true"

volumes:
  postgres-data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.56.10,rw
      device: ":/srv/nfs/swarm-data/postgres"
  
  rabbitmq-data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.56.10,rw
      device: ":/srv/nfs/swarm-data/rabbitmq"

secrets:
  db_url:
    external: true
  db_user:
    external: true
  db_password:
    external: true
  jwt_secret:
    external: true
  rabbitmq_url:
    external: true
  rabbitmq_user:
    external: true
  rabbitmq_password:
    external: true

configs:
  nginx_config:
    external: true
  gateway_config:
    external: true
```

**PrÃ©paration et dÃ©ploiement** :

```bash
# CrÃ©er les rÃ©pertoires NFS
sudo mkdir -p /srv/nfs/swarm-data/{postgres,rabbitmq}

# CrÃ©er les secrets
echo "postgresql://user:pass@postgres:5432/ecommerce" | docker secret create db_url -
echo "ecomuser" | docker secret create db_user -
echo "SecureDbPass123!" | docker secret create db_password -
echo "MyJWT_SecretKey_2024" | docker secret create jwt_secret -
echo "amqp://admin:pass@rabbitmq:5672/" | docker secret create rabbitmq_url -
echo "admin" | docker secret create rabbitmq_user -
echo "SecureRabbitPass123!" | docker secret create rabbitmq_password -

# CrÃ©er les configs
docker config create nginx_config nginx-frontend.conf
docker config create gateway_config nginx-gateway.conf

# Labelliser les nÅ“uds
docker node update --label-add storage=nfs worker1

# DÃ©ployer
docker stack deploy -c ecommerce-stack.yml ecommerce
```

**Monitoring et validation** :

```bash
# Statut de la stack
docker stack ps ecommerce

# Services
docker stack services ecommerce

# Logs
docker service logs ecommerce_products-service

# Health status
docker service ps --filter "desired-state=running" ecommerce_postgres
```

**Tests de charge et rÃ©silience** :

1. Test de montÃ©e en charge
2. Simulation de panne d'un service
3. Test de mise Ã  jour rolling
4. Validation de la persistance

**Livrables** :
- Stack complÃ¨te fonctionnelle
- Documentation d'architecture
- ProcÃ©dures de dÃ©ploiement
- Tests de rÃ©silience
- Plan de monitoring

---

## ğŸ“ Questions de SynthÃ¨se

### Architecture
1. Pourquoi 3, 5 ou 7 managers et pas 2, 4 ou 6 ?
2. Comment dimensionner le nombre de workers ?
3. Quelle stratÃ©gie pour la haute disponibilitÃ© des donnÃ©es ?

### Persistance
1. Quand utiliser des volumes locaux vs distribuÃ©s ?
2. Comment gÃ©rer les migrations de donnÃ©es ?
3. Quelle stratÃ©gie de backup pour une production critique ?

### SÃ©curitÃ©
1. DiffÃ©rence entre secrets et configs ?
2. Comment protÃ©ger les communications inter-services ?
3. StratÃ©gie de rotation des secrets ?

### OpÃ©rations
1. ProcÃ©dure de mise Ã  jour d'une stack en production ?
2. Comment gÃ©rer un rollback ?
3. StratÃ©gie de monitoring et alerting ?

## ğŸ“Š CritÃ¨res d'Ã‰valuation

| CritÃ¨re | Points | Description |
|---------|--------|-------------|
| Cluster HA (3 managers) | 15 | Configuration et validation |
| Test failover | 10 | Documentation du comportement |
| Stockage distribuÃ© | 15 | NFS ou solution Ã©quivalente |
| Stack WordPress | 15 | DÃ©ploiement avec persistance |
| Gestion secrets/configs | 10 | Utilisation appropriÃ©e |
| Backup/Restore | 10 | ProcÃ©dures testÃ©es |
| Healthchecks | 10 | Auto-healing fonctionnel |
| Stack production | 10 | Application complÃ¨te |
| Documentation | 5 | QualitÃ© des livrables |
| **Total** | **100** | |

## ğŸš€ Aller Plus Loin

1. ImplÃ©menter Consul pour le service discovery
2. Configurer GlusterFS comme stockage distribuÃ©
3. Mettre en place une stack de monitoring (Prometheus/Grafana)
4. Automatiser les backups avec des CronJobs
5. ImplÃ©menter une stratÃ©gie de disaster recovery multi-datacenter

---

**Temps estimÃ©** : 6-8 heures

**DifficultÃ©** : â­â­â­â­â˜†

**Prerequis** : Lab 1 validÃ©

**Next** : [Lab 3 - SÃ©curitÃ© et Monitoring](../lab-03-securite-monitoring/README.md)
