# Lab 1 - DÃ©couverte et Architecture Docker Swarm

## ğŸ¯ Objectifs pÃ©dagogiques

- Comprendre l'architecture distribuÃ©e de Docker Swarm
- Initialiser un cluster Swarm multi-nÅ“uds
- MaÃ®triser les concepts de managers et workers
- DÃ©couvrir les mÃ©canismes de haute disponibilitÃ©
- DÃ©ployer ses premiÃ¨res applications en mode Swarm

## ğŸ“‹ PrÃ©requis

- Environnement Vagrant configurÃ© avec 3 VMs (1 manager, 2 workers)
- AccÃ¨s SSH aux machines
- Docker installÃ© sur toutes les VMs
- Connaissances de base de Docker (images, conteneurs, volumes)

## ğŸ—ï¸ Architecture cible

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCKER SWARM CLUSTER                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   manager1   â”‚      â”‚   worker1    â”‚   â”‚   worker2    â”‚ â”‚
â”‚  â”‚  (Leader)    â”‚â—„â”€â”€â”€â”€â–ºâ”‚              â”‚   â”‚              â”‚ â”‚
â”‚  â”‚ 192.168.56.10â”‚      â”‚192.168.56.11 â”‚   â”‚192.168.56.12 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                      â”‚                   â”‚         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                    Overlay Network                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š Exercices

### Exercice 1.1 - Initialisation du Cluster

**Objectif** : CrÃ©er le cluster Swarm et comprendre son architecture

**Ã‰tapes** :

1. Connexion au manager :
```bash
vagrant ssh manager1
```

2. Initialisation du Swarm :
```bash
docker swarm init --advertise-addr 192.168.56.10
```

3. **Questions** :
   - Quelle commande affiche pour joindre le cluster ?
   - OÃ¹ est stockÃ© le token de jointure ?
   - Quel est le rÃ´le du paramÃ¨tre `--advertise-addr` ?

4. Obtenir le token worker :
```bash
docker swarm join-token worker
```

5. Obtenir le token manager :
```bash
docker swarm join-token manager
```

**Livrables** :
- Screenshot de la sortie de `docker swarm init`
- Copie des deux tokens (worker et manager)

---

### Exercice 1.2 - Ajout des Workers

**Objectif** : Joindre les nÅ“uds workers au cluster

**Ã‰tapes** :

1. Sur worker1 :
```bash
vagrant ssh worker1
docker swarm join --token SWMTKN-1-xxxxx 192.168.56.10:2377
```

2. Sur worker2 :
```bash
vagrant ssh worker2
docker swarm join --token SWMTKN-1-xxxxx 192.168.56.10:2377
```

3. VÃ©rification depuis le manager :
```bash
docker node ls
```

**Questions** :
- Combien de nÅ“uds sont listÃ©s ?
- Quel est le statut de chaque nÅ“ud ?
- Quelle est la diffÃ©rence entre `AVAILABILITY` et `STATUS` ?

**Livrables** :
- Screenshot de `docker node ls`
- RÃ©ponses aux questions dans `reponses.md`

---

### Exercice 1.3 - Inspection du Cluster

**Objectif** : Explorer la configuration et l'Ã©tat du cluster

**Commandes Ã  exÃ©cuter** :

```bash
# Informations dÃ©taillÃ©es sur le Swarm
docker info | grep -A 10 Swarm

# DÃ©tails d'un nÅ“ud spÃ©cifique
docker node inspect manager1

# Informations formatÃ©es
docker node inspect manager1 --format '{{ .Status.State }}'
docker node inspect manager1 --format '{{ .Spec.Role }}'
docker node inspect manager1 --format '{{ .ManagerStatus.Leader }}'

# Liste des nÅ“uds avec format personnalisÃ©
docker node ls --format "table {{.Hostname}}\t{{.Status}}\t{{.Availability}}\t{{.ManagerStatus}}"
```

**Questions** :
- Quel port utilise le Raft consensus ?
- Quelle est la frÃ©quence de heartbeat ?
- OÃ¹ sont stockÃ©es les donnÃ©es du Raft log ?

**Livrables** :
- Fichier `inspection-results.txt` avec les sorties
- Document `architecture-analysis.md` rÃ©pondant aux questions

---

### Exercice 1.4 - Premier Service Simple

**Objectif** : DÃ©ployer un service basique et observer sa rÃ©partition

**Ã‰tapes** :

1. CrÃ©ation du service :
```bash
docker service create \
  --name web-nginx \
  --replicas 3 \
  --publish published=8080,target=80 \
  nginx:alpine
```

2. VÃ©rification :
```bash
# Liste des services
docker service ls

# DÃ©tails du service
docker service ps web-nginx

# Logs du service
docker service logs web-nginx
```

3. Observer la rÃ©partition :
```bash
# Sur chaque nÅ“ud
docker ps
```

**Questions** :
- Comment les 3 rÃ©plicas sont-ils rÃ©partis ?
- Que se passe-t-il si vous accÃ©dez Ã  http://192.168.56.10:8080 ?
- Que se passe-t-il si vous accÃ©dez Ã  http://192.168.56.11:8080 ?
- Qu'est-ce qui permet cette rÃ©partition de charge ?

**Livrables** :
- Screenshot de `docker service ps web-nginx`
- Document expliquant le routing mesh

---

### Exercice 1.5 - Scaling et Auto-RÃ©partition

**Objectif** : Comprendre le scaling horizontal

**Ã‰tapes** :

1. Scaler le service :
```bash
docker service scale web-nginx=6
```

2. Observer la nouvelle rÃ©partition :
```bash
docker service ps web-nginx
watch -n 1 docker service ps web-nginx
```

3. Scaler vers le bas :
```bash
docker service scale web-nginx=2
```

**Questions** :
- Comment Swarm choisit-il oÃ¹ placer les nouveaux conteneurs ?
- Que devient un conteneur supprimÃ© lors du scale down ?
- Quelle stratÃ©gie utilise Swarm pour Ã©quilibrer la charge ?

**ExpÃ©rimentation** :
```bash
# Tester diffÃ©rents nombres de rÃ©plicas
docker service scale web-nginx=1
docker service scale web-nginx=10
docker service scale web-nginx=3
```

**Livrables** :
- Tableau comparatif de la rÃ©partition selon le nombre de rÃ©plicas
- Analyse de la stratÃ©gie de placement

---

### Exercice 1.6 - Mise Ã  Jour Rolling

**Objectif** : Comprendre les mises Ã  jour sans interruption

**Ã‰tapes** :

1. DÃ©ployer un service avec l'ancienne version :
```bash
docker service create \
  --name app-demo \
  --replicas 4 \
  nginx:1.20-alpine
```

2. Configurer la stratÃ©gie de mise Ã  jour :
```bash
docker service update \
  --update-parallelism 1 \
  --update-delay 10s \
  --update-failure-action rollback \
  app-demo
```

3. Effectuer la mise Ã  jour :
```bash
docker service update --image nginx:1.21-alpine app-demo
```

4. Observer en temps rÃ©el :
```bash
watch -n 1 docker service ps app-demo
```

**Questions** :
- Que signifie `--update-parallelism 1` ?
- Ã€ quoi sert `--update-delay` ?
- Que se passe-t-il avec `--update-failure-action rollback` ?

**Livrables** :
- Captures d'Ã©cran des diffÃ©rentes phases de mise Ã  jour
- Chronologie des Ã©vÃ©nements

---

### Exercice 1.7 - Gestion des Pannes

**Objectif** : Tester la rÃ©silience du cluster

**Ã‰tapes** :

1. DÃ©ployer un service :
```bash
docker service create \
  --name resilient-app \
  --replicas 6 \
  nginx:alpine
```

2. Simuler une panne d'un worker :
```bash
# Sur worker1
vagrant ssh worker1
sudo systemctl stop docker
```

3. Observer depuis le manager :
```bash
docker node ls
docker service ps resilient-app
```

4. RedÃ©marrer le worker :
```bash
sudo systemctl start docker
```

**Questions** :
- Que devient le statut du nÅ“ud worker1 ?
- Comment les rÃ©plicas sont-elles redistribuÃ©es ?
- Combien de temps prend la dÃ©tection de la panne ?
- Que se passe-t-il au redÃ©marrage du nÅ“ud ?

**Livrables** :
- Journal des Ã©vÃ©nements avec timestamps
- Analyse du temps de rÃ©cupÃ©ration (RTO)

---

### Exercice 1.8 - Labels et Contraintes

**Objectif** : ContrÃ´ler le placement des services

**Ã‰tapes** :

1. Ajouter des labels aux nÅ“uds :
```bash
docker node update --label-add environment=prod manager1
docker node update --label-add environment=dev worker1
docker node update --label-add environment=dev worker2
docker node update --label-add tier=frontend worker1
docker node update --label-add tier=backend worker2
```

2. DÃ©ployer avec contraintes :
```bash
# Service uniquement sur les workers dev
docker service create \
  --name dev-app \
  --constraint 'node.labels.environment==dev' \
  --replicas 4 \
  nginx:alpine

# Service uniquement sur le backend
docker service create \
  --name backend-service \
  --constraint 'node.labels.tier==backend' \
  --replicas 2 \
  redis:alpine
```

3. VÃ©rifier le placement :
```bash
docker service ps dev-app
docker service ps backend-service
```

**Questions** :
- OÃ¹ sont dÃ©ployÃ©es les rÃ©plicas de dev-app ?
- Que se passe-t-il si vous tentez 10 rÃ©plicas sur backend-service ?
- Comment combiner plusieurs contraintes ?

**Livrables** :
- Liste des labels appliquÃ©s
- Documentation de la stratÃ©gie de placement

---

### Exercice 1.9 - RÃ©seau Overlay

**Objectif** : CrÃ©er et utiliser des rÃ©seaux overlay

**Ã‰tapes** :

1. CrÃ©er un rÃ©seau overlay :
```bash
docker network create \
  --driver overlay \
  --subnet 10.0.9.0/24 \
  my-app-network
```

2. Lister les rÃ©seaux :
```bash
docker network ls
```

3. DÃ©ployer des services sur ce rÃ©seau :
```bash
docker service create \
  --name frontend \
  --network my-app-network \
  --replicas 3 \
  nginx:alpine

docker service create \
  --name backend \
  --network my-app-network \
  --replicas 2 \
  redis:alpine
```

4. Tester la communication :
```bash
# Depuis un conteneur frontend
docker exec -it $(docker ps -q -f name=frontend) sh
ping backend
```

**Questions** :
- Comment les conteneurs se dÃ©couvrent-ils ?
- Quel est le rÃ´le du DNS interne ?
- Quelle est la diffÃ©rence entre overlay et bridge ?

**Livrables** :
- SchÃ©ma du rÃ©seau overlay
- RÃ©sultats des tests de connectivitÃ©

---

### Exercice 1.10 - Stack Multi-Services

**Objectif** : DÃ©ployer une application complÃ¨te avec Docker Stack

**Fichier** : CrÃ©er `voting-app-stack.yml`

```yaml
version: '3.8'

services:
  vote:
    image: dockersamples/examplevotingapp_vote
    ports:
      - "5000:80"
    networks:
      - frontend
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure

  redis:
    image: redis:alpine
    networks:
      - frontend
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == worker

  worker:
    image: dockersamples/examplevotingapp_worker
    networks:
      - frontend
      - backend
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3

  db:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    networks:
      - backend
    volumes:
      - db-data:/var/lib/postgresql/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager

  result:
    image: dockersamples/examplevotingapp_result
    ports:
      - "5001:80"
    networks:
      - backend
    deploy:
      replicas: 1

networks:
  frontend:
    driver: overlay
  backend:
    driver: overlay

volumes:
  db-data:
```

**DÃ©ploiement** :

```bash
docker stack deploy -c voting-app-stack.yml voting-app
```

**VÃ©rification** :

```bash
docker stack ls
docker stack services voting-app
docker stack ps voting-app
```

**Tests** :

- AccÃ©der Ã  http://192.168.56.10:5000 (vote)
- AccÃ©der Ã  http://192.168.56.10:5001 (rÃ©sultats)

**Questions** :
- Combien de services sont crÃ©Ã©s ?
- Comment les rÃ©seaux sont-ils configurÃ©s ?
- OÃ¹ est stockÃ©e la base de donnÃ©es ?

**Livrables** :
- Fichier stack YAML commentÃ©
- Screenshots de l'application en fonctionnement
- Document d'analyse de l'architecture

---

## ğŸ“ Questions de RÃ©flexion

1. **Architecture** :
   - Quelle est la diffÃ©rence fondamentale entre Docker Compose et Docker Stack ?
   - Pourquoi a-t-on besoin de plusieurs managers ?
   - Qu'est-ce que le quorum Raft et pourquoi est-il important ?

2. **Haute DisponibilitÃ©** :
   - Comment Swarm assure-t-il la haute disponibilitÃ© des services ?
   - Que se passe-t-il si le manager leader tombe ?
   - Quelle est la diffÃ©rence entre un nÅ“ud `drain` et `pause` ?

3. **RÃ©seau** :
   - Comment fonctionne le routing mesh ?
   - Quelle est la diffÃ©rence entre mode `ingress` et mode `host` ?
   - Comment les conteneurs communiquent-ils entre diffÃ©rents nÅ“uds ?

4. **SÃ©curitÃ©** :
   - Comment sont sÃ©curisÃ©es les communications inter-nÅ“uds ?
   - Quel protocole utilise le Raft consensus ?
   - Comment sont gÃ©rÃ©s les secrets dans Swarm ?

## ğŸ“Š CritÃ¨res d'Ã‰valuation

| CritÃ¨re | Points | Description |
|---------|--------|-------------|
| Initialisation cluster | 10 | Cluster fonctionnel avec 3 nÅ“uds |
| DÃ©ploiement services | 15 | Services dÃ©ployÃ©s et accessibles |
| Scaling | 10 | Scaling up/down maÃ®trisÃ© |
| Mise Ã  jour rolling | 15 | Mise Ã  jour sans interruption |
| Gestion des pannes | 15 | Tests de rÃ©silience documentÃ©s |
| Labels et contraintes | 10 | Placement contrÃ´lÃ© des services |
| RÃ©seaux overlay | 10 | Communication inter-services |
| Stack multi-services | 10 | Application complÃ¨te dÃ©ployÃ©e |
| Documentation | 5 | Livrables complets et clairs |
| **Total** | **100** | |

## ğŸš€ Aller Plus Loin

**DÃ©fis supplÃ©mentaires** :

1. Ajouter un 4Ã¨me nÅ“ud en tant que manager
2. ImplÃ©menter un service avec placement global
3. CrÃ©er un rÃ©seau overlay chiffrÃ©
4. Tester le failover d'un manager
5. DÃ©ployer une stack avec des secrets

**Ressources** :

- [Docker Swarm Documentation](https://docs.docker.com/engine/swarm/)
- [Raft Consensus Algorithm](https://raft.github.io/)
- [Docker Networking](https://docs.docker.com/network/)

---

**Temps estimÃ©** : 4-6 heures

**DifficultÃ©** : â­â­â˜†â˜†â˜†

**Next** : [Lab 2 - Haute DisponibilitÃ© et Persistance](../lab-02-ha-persistance/README.md)
