version: '3.9'

services:
  # Traefik v3 - Reverse Proxy avec SSL/TLS
  traefik:
    image: traefik:v3.0
    container_name: harbor-traefik
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    networks:
      - public
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"
    environment:
      TRAEFIK_API_INSECURE: "false"
      TRAEFIK_API_DASHBOARD: "true"
      TRAEFIK_ENTRYPOINTS_WEB_ADDRESS: ":80"
      TRAEFIK_ENTRYPOINTS_WEBSECURE_ADDRESS: ":443"
      TRAEFIK_ENTRYPOINTS_WEBSECURE_HTTP_TLS_CERTRESOLVER: "letsencrypt"
      TRAEFIK_CERTIFICATESRESOLVERS_LETSENCRYPT_ACME_HTTPCHALLENGE: "true"
      TRAEFIK_CERTIFICATESRESOLVERS_LETSENCRYPT_ACME_HTTPCHALLENGE_ENTRYPOINT: "web"
      TRAEFIK_CERTIFICATESRESOLVERS_LETSENCRYPT_ACME_EMAIL: "${TRAEFIK_ACME_EMAIL}"
      TRAEFIK_CERTIFICATESRESOLVERS_LETSENCRYPT_ACME_STORAGE: "/acme/acme.json"
      TRAEFIK_PROVIDERS_FILE_DIRECTORY: "/traefik/config"
      TRAEFIK_PROVIDERS_FILE_WATCH: "true"
      TRAEFIK_PROVIDERS_DOCKER: "true"
      TRAEFIK_PROVIDERS_DOCKER_EXPOSEDBYDEFAULT: "false"
      TRAEFIK_PROVIDERS_DOCKER_NETWORK: "public"
      TRAEFIK_ACCESSLOG: "true"
      TRAEFIK_ACCESSLOG_FILEPATH: "/var/log/traefik/access.log"
      TRAEFIK_LOG_LEVEL: "${TRAEFIK_LOG_LEVEL:-INFO}"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./traefik/dynamic:/traefik/config
      - ./traefik/acme:/acme
      - ./traefik/logs:/var/log/traefik
    labels:
      traefik.enable: "true"
      traefik.http.routers.traefik.rule: "Host(`${TRAEFIK_DASHBOARD_HOST}`)"
      traefik.http.routers.traefik.service: "api@internal"
      traefik.http.routers.traefik.middlewares: "auth@file"
      traefik.http.middlewares.redirect-http.redirectscheme.scheme: "https"
      traefik.http.middlewares.redirect-http.redirectscheme.permanent: "true"
    healthcheck:
      test: ["CMD", "traefik", "healthcheck", "--ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # PostgreSQL Primaire - Haute Disponibilité
  postgres-primary:
    image: postgres:15-alpine
    container_name: harbor-postgres-primary
    restart: unless-stopped
    networks:
      - database
    environment:
      POSTGRES_PASSWORD: "${DB_PASSWORD}"
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_INITDB_ARGS: >-
        -c max_connections=1000
        -c shared_buffers=256MB
        -c effective_cache_size=1GB
        -c maintenance_work_mem=64MB
        -c checkpoint_completion_target=0.9
        -c wal_buffers=16MB
        -c default_statistics_target=100
        -c random_page_cost=1.1
        -c effective_io_concurrency=200
        -c max_wal_senders=10
        -c max_replication_slots=10
        -c hot_standby=on
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgresql-data:/var/lib/postgresql/data
      - ./postgres/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh:ro
      - ./postgres/postgresql.conf:/var/lib/postgresql/postgresql.conf:ro
    ports:
      - "${DB_PORT}:5432"
    command: |
      postgres
      -c config_file=/var/lib/postgresql/postgresql.conf
      -c wal_level=replica
      -c max_wal_senders=10
      -c max_replication_slots=10
      -c hot_standby=on
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PostgreSQL Réplica (optionnel, pour HA)
  postgres-replica:
    image: postgres:15-alpine
    container_name: harbor-postgres-replica
    restart: unless-stopped
    networks:
      - database
    environment:
      PGUSER: ${DB_USER:-postgres}
      PGPASSWORD: "${DB_PASSWORD}"
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgresql-replica-data:/var/lib/postgresql/data
    command: |
      bash -c "
      until pg_basebackup -h postgres-primary -D /var/lib/postgresql/data -U ${DB_USER:-postgres} -v -W -P -R -X stream -c fast;
      do
        echo 'Waiting for primary...'
        sleep 1s
      done;
      exec postgres -c hot_standby=on
      "
    depends_on:
      postgres-primary:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Master - Haute Disponibilité
  redis-master:
    image: redis:7-alpine
    container_name: harbor-redis-master
    restart: unless-stopped
    networks:
      - backend
    command: >
      redis-server
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
      --requirepass ${REDIS_PASSWORD}
      --masterauth ${REDIS_PASSWORD}
      --slaveof 127.0.0.1 6380
      --slave-read-only yes
      --min-slaves-to-write 1
      --min-slaves-max-lag 10
    volumes:
      - redis-data:/data
    ports:
      - "${REDIS_PORT}:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Sentinel 1
  redis-sentinel-1:
    image: redis:7-alpine
    container_name: harbor-redis-sentinel-1
    restart: unless-stopped
    networks:
      - backend
    command: >
      redis-sentinel /etc/redis-sentinel/sentinel.conf
      --sentinel-port 26379
      --dir /var/lib/sentinel
    volumes:
      - ./redis/sentinel-1.conf:/etc/redis-sentinel/sentinel.conf:ro
      - redis-sentinel-1:/var/lib/sentinel
    depends_on:
      redis-master:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "26379", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Sentinel 2
  redis-sentinel-2:
    image: redis:7-alpine
    container_name: harbor-redis-sentinel-2
    restart: unless-stopped
    networks:
      - backend
    command: >
      redis-sentinel /etc/redis-sentinel/sentinel.conf
      --sentinel-port 26379
      --dir /var/lib/sentinel
    volumes:
      - ./redis/sentinel-2.conf:/etc/redis-sentinel/sentinel.conf:ro
      - redis-sentinel-2:/var/lib/sentinel
    depends_on:
      redis-master:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "26379", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Sentinel 3
  redis-sentinel-3:
    image: redis:7-alpine
    container_name: harbor-redis-sentinel-3
    restart: unless-stopped
    networks:
      - backend
    command: >
      redis-sentinel /etc/redis-sentinel/sentinel.conf
      --sentinel-port 26379
      --dir /var/lib/sentinel
    volumes:
      - ./redis/sentinel-3.conf:/etc/redis-sentinel/sentinel.conf:ro
      - redis-sentinel-3:/var/lib/sentinel
    depends_on:
      redis-master:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "26379", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Harbor Core - API et Business Logic
  harbor-core:
    image: goharbor/harbor-core:v2.9.1
    container_name: harbor-core
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      postgres-primary:
        condition: service_healthy
      redis-master:
        condition: service_healthy
    environment:
      DATABASE_TYPE: postgresql
      POSTGRES_HOST: postgres-primary
      POSTGRES_PORT: 5432
      POSTGRES_DBNAME: ${DB_NAME:-harbor}
      POSTGRES_USERNAME: ${DB_USER:-postgres}
      POSTGRES_PASSWORD: "${DB_PASSWORD}"
      POSTGRES_SSLMODE: disable
      REDIS_HOST: redis-master
      REDIS_PORT: 6379
      REDIS_PASSWORD: "${REDIS_PASSWORD}"
      REDIS_DB_INDEX: 0
      CORE_SECRET: "${CORE_SECRET}"
      JOBSERVICE_SECRET: "${JOBSERVICE_SECRET}"
      INTERNAL_TLS_ENABLED: "false"
      REGISTRY_CREDENTIAL_USERNAME: "${REGISTRY_CREDENTIAL_USERNAME}"
      REGISTRY_CREDENTIAL_PASSWORD: "${REGISTRY_CREDENTIAL_PASSWORD}"
      REGISTRY_STORAGE_PROVIDER_NAME: "${REGISTRY_STORAGE_PROVIDER:-s3}"
      REGISTRY_STORAGE_S3_REGION: "${S3_REGION:-us-east-1}"
      REGISTRY_STORAGE_S3_BUCKET: "${S3_BUCKET:-harbor-registry}"
      REGISTRY_STORAGE_S3_ACCESSKEY: "${S3_ACCESS_KEY}"
      REGISTRY_STORAGE_S3_SECRETKEY: "${S3_SECRET_KEY}"
      REGISTRY_STORAGE_S3_SECURE: "true"
      REGISTRY_STORAGE_S3_V4AUTH: "true"
      LDAP_URL: "${LDAP_URL:-}"
      LDAP_SEARCHDN: "${LDAP_SEARCHDN:-}"
      LDAP_SEARCH_FILTER: "${LDAP_SEARCH_FILTER:-(uid=%s)}"
      LDAP_UID: "${LDAP_UID:-uid}"
      LDAP_SCOPE: "2"
      LDAP_TIMEOUT: "5"
      LDAP_VERIFY_CERT: "true"
      OIDC_NAME: "${OIDC_NAME:-OIDC}"
      OIDC_ENDPOINT: "${OIDC_ENDPOINT:-}"
      OIDC_CLIENT_ID: "${OIDC_CLIENT_ID:-}"
      OIDC_CLIENT_SECRET: "${OIDC_CLIENT_SECRET:-}"
      OIDC_VERIFY_CERT: "true"
      OIDC_SCOPE: "${OIDC_SCOPE:-openid,profile,email}"
      OIDC_USER_CLAIM: "${OIDC_USER_CLAIM:-name}"
      OIDC_GROUPS_CLAIM: "${OIDC_GROUPS_CLAIM:-}"
      WEBHOOK_HOST: "${WEBHOOK_HOST:-}"
      WEBHOOK_PORT: "8080"
      WEBHOOK_NAMESPACE: "harbor"
      ADMIN_INITIAL_PASSWORD: "${HARBOR_ADMIN_PASSWORD:-Harbor12345}"
      LOG_LEVEL: "${LOG_LEVEL:-info}"
      LOG_FORMAT: "json"
    volumes:
      - ./config/core/app.conf:/etc/harbor/app.conf:ro
      - harbor-core-data:/data
    labels:
      traefik.enable: "true"
      traefik.http.routers.harbor.rule: "Host(`${HARBOR_DOMAIN}`)"
      traefik.http.routers.harbor.entrypoints: "web,websecure"
      traefik.http.routers.harbor.tls.certresolver: "letsencrypt"
      traefik.http.routers.harbor.middlewares: "redirect-http@file"
      traefik.http.services.harbor.loadbalancer.server.port: "8080"
      prometheus-job: "harbor-core"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/v2.0/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Harbor Registry
  harbor-registry:
    image: goharbor/registry-photon:v2.9.1
    container_name: harbor-registry
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      - harbor-core
    environment:
      REGISTRY_STORAGE_S3_REGION: "${S3_REGION:-us-east-1}"
      REGISTRY_STORAGE_S3_BUCKET: "${S3_BUCKET:-harbor-registry}"
      REGISTRY_STORAGE_S3_ACCESSKEY: "${S3_ACCESS_KEY}"
      REGISTRY_STORAGE_S3_SECRETKEY: "${S3_SECRET_KEY}"
      REGISTRY_LOG_LEVEL: "${LOG_LEVEL:-info}"
      REGISTRY_LOG_FORMAT: "json"
    volumes:
      - ./config/registry/config.yml:/etc/registry/config.yml:ro
      - registry-data:/var/lib/registry
    labels:
      traefik.enable: "true"
      traefik.http.routers.registry.rule: "Host(`${HARBOR_DOMAIN}`) && PathPrefix(`/v2/`)"
      traefik.http.routers.registry.entrypoints: "web,websecure"
      traefik.http.routers.registry.tls.certresolver: "letsencrypt"
      traefik.http.services.registry.loadbalancer.server.port: "5000"
      prometheus-job: "harbor-registry"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/v2/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Harbor Portal - Web UI
  harbor-portal:
    image: goharbor/harbor-portal:v2.9.1
    container_name: harbor-portal
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      - harbor-core
    environment:
      CORE_BASE_HREF: "/"
      LOG_LEVEL: "${LOG_LEVEL:-info}"
    volumes:
      - ./config/portal/nginx.conf:/etc/nginx/nginx.conf:ro
    labels:
      traefik.enable: "false"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Harbor JobService
  harbor-jobservice:
    image: goharbor/harbor-jobservice:v2.9.1
    container_name: harbor-jobservice
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      postgres-primary:
        condition: service_healthy
      redis-master:
        condition: service_healthy
    environment:
      CORE_SECRET: "${CORE_SECRET}"
      JOBSERVICE_SECRET: "${JOBSERVICE_SECRET}"
      CORE_URL: "http://harbor-core:8080"
      TOKEN_SERVICE_URL: "http://harbor-core:8080/service/token"
      REGISTRY_CONTROLLER_URL: "http://harbor-registryctl:8080"
      LOG_LEVEL: "${LOG_LEVEL:-info}"
      LOG_FORMAT: "json"
      REDIS_URL: "redis://:${REDIS_PASSWORD}@redis-master:6379/2"
    volumes:
      - ./config/jobservice/config.yml:/etc/harbor/jobservice/config.yml:ro
      - harbor-jobservice-data:/var/log/jobs
    labels:
      prometheus-job: "harbor-jobservice"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/v1/stats"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Harbor RegistryCtl
  harbor-registryctl:
    image: goharbor/harbor-registryctl:v2.9.1
    container_name: harbor-registryctl
    restart: unless-stopped
    networks:
      - backend
    environment:
      CORE_SECRET: "${CORE_SECRET}"
      JOBSERVICE_SECRET: "${JOBSERVICE_SECRET}"
      CORE_URL: "http://harbor-core:8080"
      TOKEN_SERVICE_URL: "http://harbor-core:8080/service/token"
      REGISTRY_URL: "http://harbor-registry:5000"
      LOG_LEVEL: "${LOG_LEVEL:-info}"
      LOG_FORMAT: "json"
    volumes:
      - ./config/registry/config.yml:/etc/registry/config.yml:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Trivy Scanner
  harbor-trivy:
    image: goharbor/trivy-adapter-photon:v2.9.1
    container_name: harbor-trivy
    restart: unless-stopped
    networks:
      - backend
    environment:
      TRIVY_SEVERITY: "${TRIVY_SEVERITY:-HIGH,CRITICAL}"
      TRIVY_TIMEOUT: "5m"
      SCANNER_LOG_LEVEL: "${LOG_LEVEL:-info}"
      SCANNER_TRIVY_CACHE_DIR: "/home/scanner/.cache/trivy"
      SCANNER_TRIVY_REPORTS_CACHE_DIR: "/home/scanner/.cache/reports"
    volumes:
      - trivy-data:/home/scanner/.cache/trivy
      - trivy-reports:/home/scanner/.cache/reports
    labels:
      prometheus-job: "harbor-trivy"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/probe/ready"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Notary Server (optionnel)
  notary-server:
    image: goharbor/notary-server-photon:v2.9.1
    container_name: harbor-notary-server
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      postgres-primary:
        condition: service_healthy
    environment:
      NOTARY_SERVER_DB_URL: "postgres://${DB_USER:-postgres}:${DB_PASSWORD}@postgres-primary:5432/${DB_NAME:-notaryserver}?sslmode=disable"
      LOG_LEVEL: "${LOG_LEVEL:-info}"
    volumes:
      - ./config/notary/server-config.json:/etc/notary/server-config.json:ro
      - notary-server-data:/var/lib/notary
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4443/_notary_server/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Notary Signer (optionnel)
  notary-signer:
    image: goharbor/notary-signer-photon:v2.9.1
    container_name: harbor-notary-signer
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      postgres-primary:
        condition: service_healthy
    environment:
      NOTARY_SIGNER_DB_URL: "postgres://${DB_USER:-postgres}:${DB_PASSWORD}@postgres-primary:5432/${DB_NAME:-notarysigner}?sslmode=disable"
      LOG_LEVEL: "${LOG_LEVEL:-info}"
    volumes:
      - ./config/notary/signer-config.json:/etc/notary/signer-config.json:ro
      - notary-signer-data:/var/lib/notary
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7001/_notary_signer/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus - Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: harbor-prometheus
    restart: unless-stopped
    networks:
      - backend
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    labels:
      traefik.enable: "true"
      traefik.http.routers.prometheus.rule: "Host(`${TRAEFIK_DASHBOARD_HOST}`) && PathPrefix(`/prometheus/`)"
      traefik.http.routers.prometheus.entrypoints: "web,websecure"
      traefik.http.routers.prometheus.tls.certresolver: "letsencrypt"
      traefik.http.middlewares.prometheus-strip.stripprefix.prefixes: "/prometheus"
      traefik.http.routers.prometheus.middlewares: "prometheus-strip@docker,auth@file"
      traefik.http.services.prometheus.loadbalancer.server.port: "9090"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Grafana - Dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: harbor-grafana
    restart: unless-stopped
    networks:
      - backend
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: "${GRAFANA_PASSWORD}"
      GF_SECURITY_ADMIN_PASSWORD_RESET_LINK_EXPIRES_MINUTES: "10"
      GF_INSTALL_PLUGINS: "grafana-piechart-panel,grafana-worldmap-panel"
      GF_SERVER_ROOT_URL: "https://${TRAEFIK_DASHBOARD_HOST}/grafana/"
      GF_SERVER_SERVE_FROM_SUB_PATH: "true"
      GF_LOG_LEVEL: "${LOG_LEVEL:-info}"
      GF_ANALYTICS_REPORTING_ENABLED: "false"
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana-data:/var/lib/grafana
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    labels:
      traefik.enable: "true"
      traefik.http.routers.grafana.rule: "Host(`${TRAEFIK_DASHBOARD_HOST}`) && PathPrefix(`/grafana/`)"
      traefik.http.routers.grafana.entrypoints: "web,websecure"
      traefik.http.routers.grafana.tls.certresolver: "letsencrypt"
      traefik.http.middlewares.grafana-strip.stripprefix.prefixes: "/grafana"
      traefik.http.routers.grafana.middlewares: "grafana-strip@docker,auth@file"
      traefik.http.services.grafana.loadbalancer.server.port: "3000"
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Loki - Log Aggregation
  loki:
    image: grafana/loki:latest
    container_name: harbor-loki
    restart: unless-stopped
    networks:
      - backend
    command: -config.file=/etc/loki/loki-config.yml
    volumes:
      - ./loki/loki-config.yml:/etc/loki/loki-config.yml:ro
      - loki-data:/loki
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/loki/api/v1/status"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Promtail - Log Shipper
  promtail:
    image: grafana/promtail:latest
    container_name: harbor-promtail
    restart: unless-stopped
    networks:
      - backend
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
      - ./promtail/config.yml:/etc/promtail/config.yml:ro
      - /var/log:/var/log:ro
    depends_on:
      - loki
    healthcheck:
      test: ["CMD", "promtail", "-print-config-stderr=true"]
      interval: 30s
      timeout: 10s
      retries: 3

  # AlertManager - Alerting
  alertmanager:
    image: prom/alertmanager:latest
    container_name: harbor-alertmanager
    restart: unless-stopped
    networks:
      - backend
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=https://${TRAEFIK_DASHBOARD_HOST}/alertmanager/'
    volumes:
      - ./alertmanager/config.yml:/etc/alertmanager/config.yml:ro
      - alertmanager-data:/alertmanager
    labels:
      traefik.enable: "true"
      traefik.http.routers.alertmanager.rule: "Host(`${TRAEFIK_DASHBOARD_HOST}`) && PathPrefix(`/alertmanager/`)"
      traefik.http.routers.alertmanager.entrypoints: "web,websecure"
      traefik.http.routers.alertmanager.tls.certresolver: "letsencrypt"
      traefik.http.middlewares.alertmanager-strip.stripprefix.prefixes: "/alertmanager"
      traefik.http.routers.alertmanager.middlewares: "alertmanager-strip@docker,auth@file"
      traefik.http.services.alertmanager.loadbalancer.server.port: "9093"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  public:
    driver: bridge
  backend:
    driver: bridge
    internal: false
  database:
    driver: bridge
    internal: true

volumes:
  postgresql-data:
    driver: local
  postgresql-replica-data:
    driver: local
  redis-data:
    driver: local
  redis-sentinel-1:
    driver: local
  redis-sentinel-2:
    driver: local
  redis-sentinel-3:
    driver: local
  registry-data:
    driver: local
  harbor-core-data:
    driver: local
  harbor-jobservice-data:
    driver: local
  trivy-data:
    driver: local
  trivy-reports:
    driver: local
  notary-server-data:
    driver: local
  notary-signer-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  loki-data:
    driver: local
  alertmanager-data:
    driver: local
